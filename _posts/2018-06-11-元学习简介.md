---
layout:     post
title:      元学习简介
subtitle:   人工智能的未来
date:       2018-06-10
author:     徐清华
header-img: img/post-bg-ios9-web.jpg
catalog: 	 true
tags:
    - 元学习
---

# 1.Meta-Learning Origin
>Metalearning (also known as 'meta-learning' or 'learning to learn'), an old informal concept of [cognitive psychology](http://www.scholarpedia.org/article/Cognitive_psychology "Cognitive psychology"), more recently became a formal concept of machine learning. Among the earliest machine learning approaches to metalearning is a system designed to adjust bias, called *STABB* (Shift To A Better Bias), introduced by Utgoff (1986), the *Variable bias management system* by Rendell, Senshu and Tcheng (1987) which selects between different learning algorithms, and meta-genetic programming (Schmidhuber, 1987), to our knowledge the first system that tries to learn entire learning algorithms, through methods of artificial evolution.
As of 2017 the term had not found a standard interpretation, however the main goal is to use such metadata to understand how automatic learning can become flexible in solving learning problems, hence to improve the performance of existing [learning algorithms](https://en.wikipedia.org/wiki/Learning_algorithms "Learning algorithms") or to learn (induce) the learning algorithm itself, hence the alternative term **learning to learn**.

&emsp;&emsp;关于元学习的最早出现于Bengio et al.的早期工作 (1991)<sup>[[1]](https://en.wikipedia.org/wiki/Meta_learning_(computer_science)#cite_note-1)</sup>。论文中认为人类基因的进化过程就是神经网络的学习过程，并且人类的神经网络是根据人的DNA记录的信息生成的。元学习的基本思想是，让机器能够自己通过分析数据，结合以往的建模经验，建立模型，并且训练，即建模过程的自动化。
 &emsp;&emsp;[这篇博客](https://zhuanlan.zhihu.com/p/27629294)介绍了元学习的基本思路。本文借鉴一些理念进行说明。
![image.png](https://upload-images.jianshu.io/upload_images/12011882-b1a94def3b9c5d17.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
&emsp;&emsp;首先是这个开瓶子的例子。作者这个例子举得非常非常好。元学习想要解决的是能够学会学习的问题。想象一下，如果们对机器进行训练，训练了市面上所有的瓶子的开法，机器学的非常认真，效果非常好。但是，啪！第三个瓶子横空出世！人类的话，自己琢磨琢磨就能打开了。但是机器完全懵掉了，陷入到了深深的“I can help!”的循环值周。
&emsp;&emsp;这个例子之所以说好是因为它是说明了几个问题①人类能开第三个瓶子是基于过往的经验，但是机器想要开瓶子就得从新开始学习。②机器学习的过程是完全基于数据的，这容易造成学习偏倚，对于少量出现或者从来没有出现过的问题很难解决。

# 2.Defination
&emsp;&emsp;目前公认的对于元学习系统的定义认为任何一个元学习系统都具有以下三个特征：
&emsp;&emsp;&emsp;&emsp;1.元学习系统必须包含一个学习子系统。
&emsp;&emsp;&emsp;&emsp;2.经验的获取方式只能是①从之前在单个训练集中训练的结果提取出来的②从其他数据域中提取出来的
&emsp;&emsp;&emsp;&emsp;3.学习偏倚必须是动态的选择的。这里的偏倚指的模型和模型的超参数选择造成的偏倚，而不是因为数据造成的结果偏倚。元学习中考虑的偏倚主要包括两种：①声明式偏倚。在定义模型的过程中，确定了假设空间的范围，从而影响了搜索空间的大小。②过程式偏倚。在假设的选择过程中有倾向性，比如倾向于选择复杂度较小的假设。

# 3. Meta-Learning 方法介绍
>Good examples of *learning the model initial parameters* are [Model-Agnostic Meta-Learning](https://arxiv.org/abs/1703.03400) of UC Berkeley and its [recent developments](https://openreview.net/forum?id=BJ_UL-k0b) as well as the [Reptile algorithm](https://blog.openai.com/reptile/) of OpenAI. A good example of *learning the optimizer’s parameters* is the [Learning to learn by gradient descent by gradient descent](https://arxiv.org/abs/1606.04474) paper of DeepMind. A paper combining the two is the work [Optimization as a Model for Few-Shot Learning](https://openreview.net/forum?id=rJY0-Kcll) by Sachin Ravi and Hugo Larochelle. An nice and very recent overview can be found in [Learning Unsupervised Learning Rules](https://arxiv.org/abs/1804.00222).

#### 3.1 Meta-critic 方法
&emsp;&emsp;这种方法是在Actor-critic方法的基础上提出来的.该方法不是本文研究的重点因此没有过多的去了解,其主要的思想是在认为模型在建立的过程中应该遵从某一个"价值观".所谓的价值观能够做到模型的设计过程中的各种权衡.

#### 3.2 RNN 方法介绍
&emsp;&emsp;这种方法主要用于在Cnn网络的设计过程中.其主要思想是,如果我们把每一个CNN模型的层都进行了抽象,总结成单元个数,阈值等.那么我们只要把每一层的这些信息都生成,那么我们就完成了自动化建模的问题.而生成这些层次的信息过程实际上是一个序列生成的问题,那么解决序列问题的最重要的工具就是RNN系列的那些东西.

#### 3.3 Evolving方法介绍
&emsp;&emsp;进化的方法没有太多的使用价值,因为复杂度太高.但是其中心思想还是很好理解的.首先我们会给出几个简单的网络结构作为备选.然后经过训练之后在测试集上进行训练,训练的结果差的直接淘汰.训练结果好的进行遗传和变异.遗传就是保留当前的网络结构,变异就是对网络结构进行微调,产生新的网络结构.然后用这些新一代的网络结构继续进行筛选.

#### 3.4 MANNs 方法介绍
&emsp;&emsp;留白空缺

#### 3.5 Decomposing Supervised Learning to Meta Learning 

&emsp;&emsp;这一篇论文的主要任务是NATURAL LANGUAGE TO SQL.主要的贡献在于使用了元学习的概念.在本文中,对元学习的理解主要是训练多个模型总结模型经验,而不是数据样本经验.数据样本可能会造成偏倚,但是针对概率分布进行建模则不会出现类似的情况.因此首先我们需要多个任务,然后针对多个任务进行建模,然后就可以总结建模经验,用于下一次的计算和迭代.这种做法能够快速适应新的数据,有效的解决One-shot 的问题.

&emsp;&emsp;具体的过程应该包括:

&emsp;&emsp;&emsp;&emsp;1.构建数据集合.首先以每个数据样本作为数据集.然后利用关联函数求出和样本最接近的Top K的样本加入到数据集中,最终形成训练数据集.而原始的那个数据样本,作为测试数据.

&emsp;&emsp;&emsp;&emsp;2.在每个数据集合上跑任务,任务跑完求损失函数,根据损失函数进行梯度下降.注意,只根据测试样本上的损失函数求梯度下降.这里比较容易产生困扰的地方在于,在每个任务的内部都会使用梯度下降进行训练,已经更新过参数了,但是请注意:**这些修改不算数**.这些修改的唯一目的是为了能够在跑一跑求出损失函数.并不会造成真正的梯度更新.

&emsp;&emsp;&emsp;&emsp;3.多轮样本迭代更新完成之后就可以得出比较适合的参数.可以使用该模型进行预测了.


# 4.Meta-Learning & NLP
#### 4.1 Question Answering
#### 4.2 negation problem
#### 4.3 ambiguation problem
#### 4.4 natural language to generate  structured query 


# 参考博客

1.[Taxonomy of Methods for Deep Meta Learning](https://medium.com/intuitionmachine/machines-that-search-for-deep-learning-architectures-c88ae0afb6c8)
2.[From zero to research — An introduction to Meta-learning](https://medium.com/huggingface/from-zero-to-research-an-introduction-to-meta-learning-8e16e677f78a)
3.[学会学习Learning to Learn：让AI拥有核心价值观从而实现快速学习](https://zhuanlan.zhihu.com/p/27629294)
4.[最前沿：百家争鸣的Meta Learning/Learning to learn](https://zhuanlan.zhihu.com/p/28639662)
5.[What’s New in Deep Learning Research: Understanding Meta-Learning](https://towardsdatascience.com/whats-new-in-deep-learning-research-understanding-meta-learning-91fef1295660)
