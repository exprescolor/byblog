---
layout:     post
title:      Elasticsearch进阶
subtitle:   
author:     大暴马
catalog: 	 true
tags:
    - ES
    - 开发
---
### 分布式特性
Elasticsearch 尽可能地屏蔽了分布式系统的复杂性。这里列举了一些在后台自动执行的操作：

1. 分配文档到不同的容器 或 分片 中，文档可以储存在一个或多个节点中
2. 按集群节点来均衡分配这些分片，从而对索引和搜索过程进行负载均衡
3. 复制每个分片以支持数据冗余，从而防止硬件故障导致的数据丢失
4. 将集群中任一节点的请求路由到存有相关数据的节点
5. 集群扩容时无缝整合新节点，重新分配分片以便从离群节点恢复

了解ES分布式特性，需要知道
1. 集群扩容、
2. 故障转移(集群内的原理)
3. 分布式文档存储
4. 执行分布式搜索
5. 分区（shard）及其工作原理(分片内部原理) 。

### 集群扩容
#### 集群概念
一个运行中的 Elasticsearch 实例称为一个 节点，而集群是由一个或者多个拥有相同 cluster.name 配置的节点组成， 它们共同承担数据和负载的压力。
当有节点加入集群中或者从集群中移除节点时，集群将会重新平均分布所有的数据。
当一个节点被选举成为 主 节点时， 它将负责管理集群范围内的所有变更，例如增加、删除索引，或者增加、删除节点等。
而主节点并不需要涉及到文档级别的变更和搜索等操作，所以当集群只拥有一个主节点的情况下，即使流量的增加它也不会成为瓶颈。
任何节点都可以成为主节点。我们可以将请求发送到 集群中的任何节点 ，包括主节点。 
每个节点都知道任意文档所处的位置，并且能够将我们的请求直接转发到存储我们所需文档的节点。 
无论我们将请求发送到哪个节点，它都能负责从各个包含我们所需文档的节点收集回数据，并将最终结果返回給客户端。 
Elasticsearch 对这一切的管理都是透明的。
#### 集群健康
Elasticsearch 的集群监控信息中包含了许多的统计数据，其中最为重要的一项就是 集群健康 ， 它在 status 字段中展示为 green 、 yellow 或者 red。
```
GET /_cluster/health
```
```
{
   "cluster_name":          "elasticsearch",
   "status":                "green", 
   "timed_out":             false,
   "number_of_nodes":       1,
   "number_of_data_nodes":  1,
   "active_primary_shards": 0,
   "active_shards":         0,
   "relocating_shards":     0,
   "initializing_shards":   0,
   "unassigned_shards":     0
}
```
status 字段是我们最关心的,指示着当前集群在总体上是否工作正常。它的三种颜色含义如下：

颜色|状态
----|----
green|所有的主分片和副本分片都正常运行。
yellow|所有的主分片都正常运行，但不是所有的副本分片都正常运行。比如单节点集群中，副本分片实际上没有作用，不需要挂在任何的节点上。因此副本分片不会运行，也就会成为yellow状态。
red|有主分片没能正常运行。

#### 主分片和副本分片
往 Elasticsearch 添加数据时需要用到 索引 —— 保存相关数据的地方。 索引实际上是指向一个或者多个物理 分片 的 逻辑命名空间 。
一个 分片 是一个底层的 工作单元 ，它仅保存了 全部数据中的一部分。 一个分片是一个 Lucene 的实例，以及它本身就是一个完整的搜索引擎。 
我们的文档被存储和索引到分片内，但是应用程序是直接与*索引*而不是与分片进行交互。

Elasticsearch 是利用分片将数据分发到集群内各处的。分片是数据的容器，文档保存在分片内，分片又被分配到集群内的各个节点里。 
当你的集群规模扩大或者缩小时， Elasticsearch 会自动的在各节点中迁移分片，使得数据仍然均匀分布在集群里。
一个分片可以是 主 分片或者 副本 分片。 索引内任意一个文档都归属于一个主分片，所以主分片的数目决定着索引能够保存的最大数据量。

一个副本分片只是一个主分片的拷贝。 副本分片作为硬件故障时保护数据不丢失的冗余备份，并为搜索和返回文档等读操作提供服务。
在索引建立的时候就已经确定了主分片数，但是副本分片数可以随时修改。

#### 创建拥有一个索引的单节点集群
```jshelllanguage
curl -X PUT "localhost:9200/blogs" -H 'Content-Type: application/json' -d'
{
   "settings" : {
      "number_of_shards" : 3,
      "number_of_replicas" : 1
   }
}
'
```
创建后的结果如图，所有3个主分片都被分配在 Node 1。
![](https://yabaowang.github.io/img/tech/ES_single_index_single_node_cluster.png)
再次查询集群健康(方法参考上文)，将得到如下结果：
![](https://yabaowang.github.io/img/tech/ES_cluster_health.png)
说明：
1. 集群 status 值为 yellow 。
2. "unassigned_shards": 3, 表示没有被分配到任何节点的副本数。
集群的健康状况为 yellow 则表示全部 主 分片都正常运行（集群可以正常服务所有请求），但是 副本 分片没有全部处在正常状态。
 实际上，所有3个副本分片都是 unassigned —— 它们都没有被分配到任何节点。 在同一个节点上既保存原始数据又保存副本是没有意义的，因为一旦失去了那个节点，我们也将丢失该节点上的所有副本数据。

当前我们的集群是正常运行的，但是在**硬件故障时有丢失数据的风险**。

### 添加第二个节点--故障转移
当集群中只有一个节点在运行时，意味着会有一个单点故障问题——没有冗余。 幸运的是，我们只需再启动一个节点即可防止数据丢失。
#### 启动第二个节点
1. 在同一台机器上启动了第二个节点时，只要它和第一个节点有同样的 cluster.name 配置，它就会自动发现集群并加入到其中。 
2. 但是在不同机器上启动节点的时候，为了加入到同一集群，你需要配置一个可连接到的单播主机列表。

我们按照1的方式，在本机启动两个ES进程：
a)将ES文件夹复制成两份，相当于制作了node1，node2的启动脚本基础
b)分别修改两个ES文件夹中，config文件夹内的elasticsearch.yml，如下：

端口号|cluster.name|node.name|path.data|path.logs
----|----|----|----|----
9200|zsh-ES-laptop|node-1|D:\\logs\\ES-node1\\data|D:\\logs\\ES-node1\\logs
9201|zsh-ES-laptop|node-2|D:\\logs\\ES-node2\\logs|D:\\logs\\ES-node2\\logs

注意数据目录和日志目录都需*手动创建*

![](https://yabaowang.github.io/img/tech/ES_two_nodes_cluster.png)

当第二个节点加入到集群后，3个 副本分片 将会分配到这个节点上——每个主分片对应一个副本分片。 这意味着当集群内**任何一个节点出现问题时，我们的数据都完好无损(故障转移)**。
所有新近被索引的文档都将会保存在主分片上，然后被并行的复制到对应的副本分片上。这就保证了我们既可以从主分片又可以从副本分片上获得文档。
cluster-health 现在展示的状态为 green ，这表示所有6个分片（包括3个主分片和3个副本分片）都在正常运行。

启停node2，作为主节点的node1会动态监测到node2的加入和离开，来采取相应措施：
![](https://yabaowang.github.io/img/tech/ES_adding_2nd_node.png)

#### 添加更多节点--水平扩容
为了分散负载而对*分片*进行重新分配

按照上述步骤，我们再启动一个node3：

端口号|cluster.name|node.name|path.data|path.logs
----|----|----|----|----
9202|zsh-ES-laptop|node-3|D:\\logs\\ES-node3\\data|D:\\logs\\ES-node3\\logs

会发现，作为主节点的node1，已经动态检测到了node3的加入：
![](https://yabaowang.github.io/img/tech/ES_adding_3rd_node.png)

随着node3加入，集群将重新分配分片数据，使得每一个node的压力减小，以提高性能：
![](https://yabaowang.github.io/img/tech/ES_re_locate.png)

Node 1 和 Node 2 上各有一个分片被迁移到了新的 Node 3 节点，现在每个节点上都拥有2个分片，而不是之前的3个。 这表示每个节点的硬件资源（CPU, RAM, I/O）将被更少的分片所共享，每个分片的性能将会得到提升。

**分片是一个功能完整的搜索引擎，它拥有使用一个节点上的所有资源的能力**
 我们这个拥有6个分片（3个主分片和3个副本分片）的索引可以*最大扩容到6个节点*，每个节点上存在一个分片，并且每个分片拥有所在节点的全部资源。

#### 更多的扩容，意味着更好的性能
但是如果我们想要扩容超过6个节点怎么办呢？

主分片的数目在索引创建时 就已经确定了下来。实际上，这个数目定义了这个索引能够 存储 的最大数据量。（实际大小取决于你的数据、硬件和使用场景。） 但是，读操作——搜索和返回数据——可以同时被主分片 或 副本分片所处理，所以当你拥有越多的副本分片时，也将拥有越高的吞吐量。

在运行中的集群上是可以动态调整副本分片数目的 ，我们可以按需伸缩集群。让我们把副本数从默认的 1 增加到 2 ：

```jshelllanguage
curl -X PUT "localhost:9200/blogs/_settings" -H 'Content-Type: application/json' -d'
{
   "number_of_replicas" : 2
}
'
```
blogs 索引现在拥有9个分片：3个主分片和6个副本分片。 这意味着我们可以将集群扩容到9个节点，每个节点上一个分片。相比原来3个节点时，集群搜索性能可以提升 3 倍。

当然，如果只是在相同节点数目的集群上增加更多的副本分片并不能提高性能，因为每个分片从节点上获得的资源会变少。 你需要增加更多的硬件资源来提升吞吐量。

但是更多的副本分片数提高了数据冗余量：按照上面的节点配置，我们可以在失去2个节点的情况下不丢失任何数据。

### 应对故障
试想，如果此时忽然关闭一个节点，会怎么样。
我们关闭的节点是一个主节点。而集群必须拥有一个主节点来保证正常工作，所以发生的第一件事情就是选举一个新的主节点： Node 2 。

在我们关闭 Node 1 的同时也失去了主分片 1 和 2 ，并且在缺失主分片的时候索引也不能正常工作。 如果此时来检查集群的状况，我们看到的状态将会为 red ：不是所有主分片都在正常工作。

幸运的是，在其它节点上存在着这两个主分片的完整副本， 所以新的主节点立即将这些分片在 Node 2 和 Node 3 上对应的副本分片提升为主分片， 此时集群的状态将会为 yellow 。 这个提升主分片的过程是瞬间发生的，如同按下一个开关一般。

为什么我们集群状态是 yellow 而不是 green 呢？ 虽然我们拥有所有的三个主分片，但是同时设置了每个主分片需要对应2份副本分片，而此时只存在一份副本分片， 所以集群不能为 green 的状态。
不过我们不必过于担心：如果我们同样关闭了 Node 2 ，我们的程序 依然 可以保持在不丢任何数据的情况下运行，因为 Node 3 为每一个分片都保留着一份副本。

如果我们重新启动 Node 1 ，集群可以将缺失的副本分片再次进行分配。 如果 Node 1 依然拥有着之前的分片，它将*尝试去重用*它们，同时**仅从主分片复制发生了修改的数据文件**。

### 微信扫一下，最新最全最好看的电影都有，谢谢了~
 ![](https://open.weixin.qq.com/qr/code?username=zhihuishangye)