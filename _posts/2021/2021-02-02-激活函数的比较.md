---
layout:     post
title:      激活函数的比较
date:       2021-02-02
author:     Yukun SHANG
catalog: 	 true
tags:
    - Deep Learning
---



# 激活函数的比较

其实这是很基础的一个内容，但是之前没有静下心来去比较各个激活函数的优缺点。

最下方的链接中有详细介绍，这里只抛出结论：

发展历程是：`Sigmoid -> Tanh -> ReLU -> Leaky ReLU -> Maxout`

目前Sigmoid用的不多， **ReLU**及其扩展以及**Softmax**（Sigmoid的扩展）用的比较多



### Reference

https://zhuanlan.zhihu.com/p/32610035