---
layout:     post
title:      DyNet-Dynamic Convolution for Accelerating Convolutional Neural Networks
subtitle:   动态生成卷积介绍
date:       2020-05-15
author:     JY
header-img: img/post-bg.jpg
catalog: true
tags:
    - paper
---



### 1. Introduction

> 提出了一种动态卷积方法，能够根据**图片内容**自适应生成卷积核 (propose a novel dynamic convolution method to adaptively generate convolution kernels based on **image contents**)

#### 背景分析

- 高性能的CNN模型需要大量计算资源 (modern high-performance CNNs often require a lot of computation resources to execute a large amount of convolution kernel operations)
- 轻量型模型 (based on efficient network design and model compression) 卷积内核(kernel)之间存在重要的相关性，因此很难在不损失信息的情况下压缩卷积核

#### 相关工作

- efficient convolution neural network design

  > SqueezeNet, Xception, MobileNet, ShuffleNet ... (详见[轻量化模型总结](https://zju-cvs.github.io/2020/04/24/lightweight-model/))

  

- model compression

  > Knowledge distillation, Pruning ...

  

- dynamic convolution kernel

  > **CV domain**: directly generate convolution kernels via a linear layer based on the feature maps of previous layers.  (卷积核中具有大量参数，因此线性层在硬件上效率不高)
  >
  > 
  >
  > **NLP domain:**
  >
  > -  incorporate context information to generate input-aware convolution ﬁlters (这些方法还是通过线性层等直接生成卷积核，由于NLP中CNN较小，而卷积核的维数为1，因此减轻了效率问题)
  > - utilizing the depthwise convolution and the strategy of sharing weight across layers (旨在提高语言建模的适应性和灵活性，而不是关注降低冗余计算成本)

#### 创新点

- 提出了动态卷积 (dynamic convolution) 来实现神经网络的加速，降低冗余计算成本

  


### 2. Method

##### 卷积核相关性

> 卷积核在深层模型中具有相关性，对经典的网络绘制特征图之间的Pearson乘积矩相关系数分布（S, M, W, N denote strong, middle, weak and no correlation respectively）可以看出：
>
> - Vgg网络特征图之间具有较强的相关性，因此存在较多冗余计算
> - 现有工作通过压缩网络来减少相关性，而压缩MobileNet这类小网络的难度大
> - 同时这些相关性是有必要的，有助于获取有噪声无关的特征（获取噪声无关的特征的过程需要多个相关的卷积核共同参与，很难在不降低信息损失的情况下压缩传统卷积核）
>
> ![img](https://github.com/ZJU-CVs/zju-cvs.github.io/raw/master/img/2020-05-19-Dynet/3.png)



##### 动态卷积

> 动态卷积的目标是学习一组内核系数，这些系数将多个固定的内核融合为一个动态的内核

 

> 动态卷积的总体架构如下图所示，它由一个系数预测模块(*coefficient prediction module*)和一个动态生成模块(*dynamic generation module*)组成。
>
> - **系数预测模块**是可训练的，旨在预测固定卷积核的系数
>
> - **动态生成模块**根据预测是系数进一步生成动态内核 
>
>   
>
>   ![img](https://github.com/ZJU-CVs/zju-cvs.github.io/raw/master/img/2020-05-19-Dynet/1.png)



###### 协方差预测模块

> **思想：基于图像内容来预测系数**
>
> 
>
> 协方差预测模块由一个全局pooling（GAP）和一个以sigmoid作为激活函数的全连接层组成
>
> 对于生成卷积核，GAP将输入的特征图聚合为$1\times1\times C_{in}$的向量，用于特征提取层；全连接层将特征进一步映射到$1\times 1\times C$的向量作为动态卷积层的固定卷积核系数

![img](https://github.com/ZJU-CVs/zju-cvs.github.io/raw/master/img/2020-05-19-Dynet/2.png)

######动态生成模块

> - 对于权重为$[C_{out}\times g_t,C_{in},k,k]$的动态卷积层，对应于$C_{out}\times g_t$的固定内核和$C_{out}$的动态内核，每个内核的大小为$[C_{in},k,k]$，$g_t$表示组的大小，是一个超参
> - 将固定的内核表示为$w_t^i$，将动态内核表示为$\widetilde{w_{t}}$，系数为$\eta_{t}^{i}$
>
> - 在获得系数后，生成动态内核：$$\widetilde{w}_{t}=\sum_{i=1}^{g_{t}} \eta_{t}^{i} \cdot w_{t}^{i}$$



**训练算法**

> 动态卷积的训练不适用于基于批量的训练方案，因为每个小批量的不同输入图像，其卷积核不同。故在训练中是基于相关系数而不是基于核进行多个特征图的融合，其在数学上是等效的，见下式所示：
>
> 
> $$
> \begin{aligned}
> \widetilde{O}_{t}
> =& \widetilde{w}_{t} \otimes x \\
> =& \sum_{i=1}^{g t} \eta_{t}^{i} \cdot w_{t}^{i} \otimes x \\
> =& \sum_{i=1}^{g t}\left(\eta_{t}^{i} \cdot w_{t}^{i} \otimes x\right) \\
> =& \sum_{i=1}^{g t}\left(\eta_{t}^{i} \cdot\left(w_{t}^{i} \otimes x\right)\right) \\
> =& \sum_{i=1}^{t}\left(\eta_{t}^{i} \cdot O_{t}^{i}\right)
> \end{aligned}
> $$
>

### 3. Experiments

- 在相近的计算量下，Dy-MobileNetv3-small的性能更高；在性能接近的情况下，Dy-ResNet50的计算量减少了三分之二

  > ![img](https://github.com/ZJU-CVs/zju-cvs.github.io/raw/master/img/2020-05-19-Dynet/5.png)

- 在MobileNet模型中，与常规内核相比，生成的动态内核具有更小的相关性

  > ![img](https://github.com/ZJU-CVs/zju-cvs.github.io/raw/master/img/2020-05-19-Dynet/4.png)

